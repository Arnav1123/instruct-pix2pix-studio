import torch
import gc
import os
from diffusers import StableDiffusionInstructPix2PixPipeline

_pipeline = None
_device = None
_device_type = None  # "cuda", "directml", "cpu"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Ryzen 5950X (16 —è–¥–µ—Ä / 32 –ø–æ—Ç–æ–∫–∞)
NUM_THREADS = 28
NUM_INTEROP = 12


def get_device():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: CUDA/ROCm > DirectML > CPU"""
    global _device, _device_type
    if _device is not None:
        return _device
    
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA (NVIDIA) –∏–ª–∏ ROCm (AMD Linux)
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"üéÆ GPU –Ω–∞–π–¥–µ–Ω: {device_name} ({vram:.1f}GB VRAM)")
        _device = torch.device("cuda")
        _device_type = "cuda"
        return _device
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º DirectML (AMD Windows)
    try:
        import torch_directml
        dml_device = torch_directml.device()
        print(f"üéÆ DirectML GPU –Ω–∞–π–¥–µ–Ω (AMD Windows)")
        _device = dml_device
        _device_type = "directml"
        return _device
    except ImportError:
        pass
    except Exception as e:
        print(f"‚ö†Ô∏è DirectML –æ—à–∏–±–∫–∞: {e}")
    
    # 3. Fallback –Ω–∞ CPU
    print("üíª GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU (Ryzen 5950X)")
    _device = torch.device("cpu")
    _device_type = "cpu"
    return _device


def get_device_type():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: cuda, directml, cpu"""
    global _device_type
    if _device_type is None:
        get_device()
    return _device_type


def setup_cpu_optimizations():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ PyTorch –¥–ª—è Ryzen 5950X"""
    torch.set_num_threads(NUM_THREADS)
    torch.set_num_interop_threads(NUM_INTEROP)
    
    os.environ["OMP_NUM_THREADS"] = str(NUM_THREADS)
    os.environ["MKL_NUM_THREADS"] = str(NUM_THREADS)
    
    print(f"üîß CPU: {NUM_THREADS} threads, {NUM_INTEROP} interop")


def clear_memory():
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
    gc.collect()
    gc.collect()
    
    # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def load_pipeline():
    """Load InstructPix2Pix pipeline - GPU preferred, CPU fallback"""
    global _pipeline

    if _pipeline is not None:
        return _pipeline
    
    device = get_device()
    device_type = get_device_type()
    
    print("üöÄ Loading InstructPix2Pix pipeline...")
    print("‚è≥ First load downloads ~5GB model, please wait...")
    
    # –í—Å–µ–≥–¥–∞ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º CPU (–¥–ª—è fallback –∏ data loading)
    setup_cpu_optimizations()
    clear_memory()
    
    try:
        if device_type == "cuda":
            # CUDA/ROCm —Ä–µ–∂–∏–º
            print("üéÆ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ CUDA/ROCm GPU (float16)...")
            
            _pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
                "timbrooks/instruct-pix2pix",
                torch_dtype=torch.float16,
                safety_checker=None,
                low_cpu_mem_usage=True,
            )
            _pipeline = _pipeline.to(device)
            
            _pipeline.enable_attention_slicing("auto")
            _pipeline.enable_vae_slicing()
            
            try:
                _pipeline.enable_vae_tiling()
                print("‚úÖ VAE tiling –≤–∫–ª—é—á–µ–Ω")
            except Exception:
                pass
            
            print("‚úÖ Pipeline –Ω–∞ GPU! (~5-15 —Å–µ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)")
            
        elif device_type == "directml":
            # DirectML —Ä–µ–∂–∏–º (AMD Windows) - float32 –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω!
            print("üéÆ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ DirectML (AMD Windows, float32)...")
            
            _pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
                "timbrooks/instruct-pix2pix",
                torch_dtype=torch.float32,  # DirectML –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç float16 –ø–æ–ª–Ω–æ—Å—Ç—å—é
                safety_checker=None,
                low_cpu_mem_usage=True,
            )
            _pipeline = _pipeline.to(device)
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è DirectML
            _pipeline.enable_attention_slicing(1)
            
            print("‚úÖ Pipeline –Ω–∞ DirectML! (~20-40 —Å–µ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)")
            
        else:
            # CPU —Ä–µ–∂–∏–º
            print("üíª –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU...")
            
            _pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
                "timbrooks/instruct-pix2pix",
                torch_dtype=torch.float32,
                safety_checker=None,
                low_cpu_mem_usage=True,
            )
            
            _pipeline.enable_attention_slicing(1)
            _pipeline.enable_vae_slicing()
            
            print("‚úÖ Pipeline –Ω–∞ CPU (~1-2 –º–∏–Ω –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)")
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        print("üîÑ –ü—Ä–æ–±—É–µ–º CPU fallback...")
        
        clear_memory()
        _pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
            "timbrooks/instruct-pix2pix",
            torch_dtype=torch.float32,
            safety_checker=None,
            low_cpu_mem_usage=True,
        )
        _pipeline.enable_attention_slicing(1)
        _pipeline.enable_vae_slicing()
        
        global _device_type
        _device_type = "cpu"
        print("‚úÖ Pipeline –Ω–∞ CPU (fallback)")
    
    _pipeline.set_progress_bar_config(disable=None)
    
    return _pipeline


def get_pipeline():
    """Get the cached pipeline instance"""
    global _pipeline
    if _pipeline is None:
        return load_pipeline()
    return _pipeline


def is_gpu_mode():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –Ω–∞ GPU"""
    return get_device_type() in ("cuda", "directml")
