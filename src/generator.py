"""
–ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–∞–¥–µ–Ω–∏–π
"""
import torch
import gc
import time
import traceback
import gradio as gr
from PIL import Image
from .pipeline import get_pipeline, clear_memory, is_gpu_mode, get_device_type, get_device
from .storage import save_image, save_to_favorites, log_generation

# –ò—Å—Ç–æ—Ä–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π (–≤ –ø–∞–º—è—Ç–∏)
_generation_history = []
_is_generating = False
_generation_queue = []

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ —Ç–∏–ø—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
MAX_STEPS = {
    "cuda": 50,
    "directml": 35,
    "cpu": 30,
}

IMAGE_SIZE = {
    "cuda": 512,
    "directml": 448,
    "cpu": 384,
}

TIME_PER_STEP = {
    "cuda": 0.3,
    "directml": 1.0,
    "cpu": 3.0,
}


def get_system_info():
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ"""
    device_type = get_device_type()
    mode_names = {
        "cuda": "üéÆ CUDA/ROCm GPU",
        "directml": "üéÆ DirectML (AMD)",
        "cpu": "üíª CPU (Ryzen)"
    }
    return {
        "device": mode_names.get(device_type, "Unknown"),
        "device_type": device_type,
        "max_steps": MAX_STEPS.get(device_type, 25),
        "image_size": IMAGE_SIZE.get(device_type, 384),
        "time_per_step": TIME_PER_STEP.get(device_type, 3.0),
    }


def estimate_time(steps):
    """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    device_type = get_device_type()
    time_per_step = TIME_PER_STEP.get(device_type, 3.0)
    total_seconds = steps * time_per_step + 5
    
    if total_seconds < 60:
        return f"~{int(total_seconds)} —Å–µ–∫"
    else:
        minutes = int(total_seconds // 60)
        seconds = int(total_seconds % 60)
        return f"~{minutes} –º–∏–Ω {seconds} —Å–µ–∫"


def safe_generate(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    def wrapper(*args, **kwargs):
        global _is_generating
        
        if _is_generating:
            return None, _generation_history[-10:] if _generation_history else [], "‚ö†Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–∂–µ –∏–¥—ë—Ç, –ø–æ–¥–æ–∂–¥–∏..."
        
        _is_generating = True
        try:
            return func(*args, **kwargs)
        except torch.cuda.OutOfMemoryError:
            clear_memory()
            return None, _generation_history[-10:] if _generation_history else [], "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU!"
        except MemoryError:
            clear_memory()
            return None, _generation_history[-10:] if _generation_history else [], "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM!"
        except Exception as e:
            clear_memory()
            print(f"‚ùå –û—à–∏–±–∫–∞:\n{traceback.format_exc()}")
            return None, _generation_history[-10:] if _generation_history else [], f"‚ùå –û—à–∏–±–∫–∞: {str(e)[:200]}"
        finally:
            _is_generating = False
            clear_memory()
    
    return wrapper


@safe_generate
def generate_image(
    image1, image2, prompt, negative_prompt, seed,
    image_guidance_scale, guidance_scale, num_inference_steps,
    auto_save=True,
    progress=gr.Progress(track_tqdm=True)
):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    global _generation_history
    
    start_time = time.time()
    
    if not prompt.strip():
        return None, _generation_history[-10:] if _generation_history else [], "‚ö†Ô∏è –í–≤–µ–¥–∏ –ø—Ä–æ–º–ø—Ç"
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    if image1 is not None:
        input_image = Image.fromarray(image1).convert("RGB")
    elif image2 is not None:
        input_image = Image.fromarray(image2).convert("RGB")
    else:
        return None, _generation_history[-10:] if _generation_history else [], "‚ö†Ô∏è –ó–∞–≥—Ä—É–∑–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device_type = get_device_type()
    image_size = IMAGE_SIZE.get(device_type, 384)
    max_steps = MAX_STEPS.get(device_type, 25)
    time_per_step = TIME_PER_STEP.get(device_type, 3.0)
    
    mode_names = {
        "cuda": "CUDA/ROCm GPU",
        "directml": "DirectML (AMD)",
        "cpu": "CPU (Ryzen 5950X)"
    }
    mode_name = mode_names.get(device_type, "Unknown")
    
    # Resize
    original_size = input_image.size
    input_image = input_image.resize((image_size, image_size), Image.Resampling.LANCZOS)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —à–∞–≥–∏
    safe_steps = min(int(num_inference_steps), max_steps)
    
    progress(0.05, desc=f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ({mode_name})...")
    clear_memory()
    time.sleep(0.3)
    
    # Seed
    actual_seed = seed if seed >= 0 else torch.randint(0, 2**32 - 1, (1,)).item()
    generator = torch.Generator("cpu").manual_seed(actual_seed)
    
    progress(0.1, desc=f"üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ({safe_steps} —à–∞–≥–æ–≤)...")
    
    pipeline = get_pipeline()
    
    # Callback –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    def progress_callback(pipe, step, timestep, callback_kwargs):
        remaining = (safe_steps - step) * time_per_step
        pct = 0.1 + (step / safe_steps) * 0.85
        progress(pct, desc=f"üé® –®–∞–≥ {step}/{safe_steps} | ~{int(remaining)} —Å–µ–∫")
        return callback_kwargs
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    if device_type == "cuda":
        with torch.inference_mode():
            with torch.cuda.amp.autocast(dtype=torch.float16):
                output = pipeline(
                    prompt=prompt,
                    image=input_image,
                    num_inference_steps=safe_steps,
                    guidance_scale=guidance_scale,
                    image_guidance_scale=image_guidance_scale,
                    generator=generator,
                    callback_on_step_end=progress_callback,
                )
    else:
        with torch.no_grad():
            output = pipeline(
                prompt=prompt,
                image=input_image,
                num_inference_steps=safe_steps,
                guidance_scale=guidance_scale,
                image_guidance_scale=image_guidance_scale,
                generator=generator,
                callback_on_step_end=progress_callback,
            )
    
    progress(0.98, desc="‚ú® –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è...")
    
    result_image = output.images[0]
    elapsed_time = time.time() - start_time
    
    # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    saved_path = None
    if auto_save:
        saved_path = save_image(result_image)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    log_generation({
        "prompt": prompt,
        "seed": actual_seed,
        "steps": safe_steps,
        "guidance": guidance_scale,
        "image_cfg": image_guidance_scale,
        "time": round(elapsed_time, 1),
        "device": mode_name,
        "saved_path": saved_path
    })
    
    # –ò—Å—Ç–æ—Ä–∏—è
    _generation_history.append(result_image)
    if len(_generation_history) > 20:
        _generation_history = _generation_history[-20:]
    
    progress(1.0, desc="‚úÖ –ì–æ—Ç–æ–≤–æ!")
    
    status = f"""‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!

‚è±Ô∏è –í—Ä–µ–º—è: {elapsed_time:.1f} —Å–µ–∫
üé≤ Seed: {actual_seed}
üìù –ü—Ä–æ–º–ø—Ç: {prompt[:60]}{'...' if len(prompt) > 60 else ''}

‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {safe_steps} —à–∞–≥–æ–≤ | CFG {guidance_scale} | ImgCFG {image_guidance_scale}
üéÆ –†–µ–∂–∏–º: {mode_name} ({image_size}px)
üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved_path if saved_path else '–ù–µ—Ç'}"""
    
    return result_image, _generation_history[-10:], status


def generate_batch(
    image, prompt, num_variations, base_seed,
    image_guidance_scale, guidance_scale, num_inference_steps,
    progress=gr.Progress()
):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–π —Å —Ä–∞–∑–Ω—ã–º–∏ seed"""
    global _generation_history
    
    if image is None:
        return [], "‚ö†Ô∏è –ó–∞–≥—Ä—É–∑–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
    
    if not prompt.strip():
        return [], "‚ö†Ô∏è –í–≤–µ–¥–∏ –ø—Ä–æ–º–ø—Ç"
    
    results = []
    num_variations = min(int(num_variations), 8)  # –ú–∞–∫—Å–∏–º—É–º 8
    
    for i in range(num_variations):
        progress((i / num_variations), desc=f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {i+1}/{num_variations}...")
        
        seed = base_seed + i if base_seed >= 0 else -1
        
        result, _, _ = generate_image(
            image, None, prompt, "", seed,
            image_guidance_scale, guidance_scale, num_inference_steps,
            auto_save=True,
            progress=progress
        )
        
        if result is not None:
            results.append(result)
    
    progress(1.0, desc="‚úÖ Batch –≥–æ—Ç–æ–≤!")
    
    return results, f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(results)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"


def use_as_input(image):
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    if image is None:
        return None
    return image


def add_to_favorites(image):
    """–î–æ–±–∞–≤–∏—Ç—å –≤ –∏–∑–±—Ä–∞–Ω–Ω–æ–µ"""
    if image is None:
        return "‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    
    try:
        if hasattr(image, 'save'):
            path = save_to_favorites(image)
        else:
            img = Image.fromarray(image)
            path = save_to_favorites(img)
        return f"‚≠ê –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∏–∑–±—Ä–∞–Ω–Ω–æ–µ: {path}"
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞: {e}"


def export_image(image, format_choice, quality):
    """–≠–∫—Å–ø–æ—Ä—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –≤—ã–±–æ—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–∞"""
    if image is None:
        return None, "‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    
    try:
        if hasattr(image, 'save'):
            img = image
        else:
            img = Image.fromarray(image)
        
        fmt = "PNG" if format_choice == "PNG" else "JPEG"
        path = save_image(img, format=fmt, quality=int(quality))
        return path, f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {path}"
    except Exception as e:
        return None, f"‚ùå –û—à–∏–±–∫–∞: {e}"


def randomize_seed():
    """–°–ª—É—á–∞–π–Ω—ã–π seed"""
    return torch.randint(0, 2**32 - 1, (1,)).item()


def clear_history():
    """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"""
    global _generation_history
    _generation_history = []
    clear_memory()
    return [], "üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞"


def get_history():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –∏—Å—Ç–æ—Ä–∏—é"""
    return _generation_history[-10:] if _generation_history else []
